{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e12a9561-0285-4a87-8e18-1090baca7bf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/geopandas/_compat.py:111: UserWarning: The Shapely GEOS version (3.10.2-CAPI-1.16.0) is incompatible with the GEOS version PyGEOS was compiled with (3.10.1-CAPI-1.16.0). Conversions between both will be slow.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from numba import njit, typed, types\n",
    "from concurrent.futures import ThreadPoolExecutor,as_completed\n",
    "from subprocess import run\n",
    "from os.path import splitext\n",
    "import rasterio\n",
    "import fiona\n",
    "from shapely.geometry import shape\n",
    "from rasterio.mask import mask\n",
    "from rasterio.io import DatasetReader,DatasetWriter\n",
    "from collections import OrderedDict\n",
    "import argparse\n",
    "from warnings import warn\n",
    "from osgeo.gdal import BuildVRT\n",
    "import geopandas as gpd\n",
    "import sys\n",
    "import xarray as xr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "633636ff-3de4-410b-997e-79d60bace123",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] -r REM -c CATCHMENTS -b CATCHMENT_POLY -t\n",
      "                             HYDRO_TABLE -f FORECAST [-u HUCS]\n",
      "                             [-l HUCS_LAYERNAME] [-j NUM_WORKERS]\n",
      "                             [-s SUBSET_HUCS [SUBSET_HUCS ...]] [-m MASK_TYPE]\n",
      "                             [-a] [-i INUNDATION_RASTER]\n",
      "                             [-p INUNDATION_POLYGON] [-d DEPTHS]\n",
      "                             [-n SRC_TABLE] [-q]\n",
      "ipykernel_launcher.py: error: the following arguments are required: -r/--rem, -c/--catchments, -b/--catchment-poly, -t/--hydro-table\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/IPython/core/interactiveshell.py:3405: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "class hydroTableHasOnlyLakes(Exception): \n",
    "    \"\"\" Raised when a Hydro-Table only has lakes \"\"\"\n",
    "    pass\n",
    "\n",
    "\n",
    "class NoForecastFound(Exception):\n",
    "    \"\"\" Raised when no forecast is available for a given Hydro-Table \"\"\"\n",
    "    pass\n",
    "\n",
    "class hydroTableHasOnlyLakes(Exception): \n",
    "    \"\"\" Raised when a Hydro-Table only has lakes \"\"\"\n",
    "    pass\n",
    "\n",
    "\n",
    "class NoForecastFound(Exception):\n",
    "    \"\"\" Raised when no forecast is available for a given Hydro-Table \"\"\"\n",
    "    pass\n",
    "\n",
    "def inundate(rem, catchments, catchment_poly, hydro_table, forecast,\n",
    "             mask_type, hucs = None, hucs_layerName = None,\n",
    "             subset_hucs = None, num_workers = 1, aggregate = False, \n",
    "             inundation_raster = None, inundation_polygon = None,\n",
    "             depths = None, out_raster_profile = None, out_vector_profile = None,\n",
    "             src_table = None, quiet = False):\n",
    "    \"\"\"\n",
    "\n",
    "    Run inundation on FIM >=3.0 outputs at job-level scale or aggregated scale\n",
    "\n",
    "    Generate depths raster, inundation raster, and inundation polygon from FIM >=3.0 outputs. Can use the FIM 3.0 outputs at native HUC level or the aggregated products. Be sure to pass a HUCs file to process in batch mode if passing aggregated products.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    rem : str or rasterio.DatasetReader\n",
    "        File path to or rasterio dataset reader of Relative Elevation Model raster. Must have the same CRS as catchments raster.\n",
    "    catchments : str or rasterio.DatasetReader\n",
    "        File path to or rasterio dataset reader of Catchments raster. Must have the same CRS as REM raster\n",
    "    hydro_table : str or pandas.DataFrame\n",
    "        File path to hydro-table csv or Pandas DataFrame object with correct indices and columns.\n",
    "    forecast : str or pandas.DataFrame\n",
    "        File path to forecast csv or Pandas DataFrame with correct column names.\n",
    "    hucs : str or fiona.Collection, optional\n",
    "        Batch mode only. File path or fiona collection of vector polygons in HUC 4,6,or 8's to inundate on. Must have an attribute named as either \"HUC4\",\"HUC6\", or \"HUC8\" with the associated values.\n",
    "    hucs_layerName : str, optional\n",
    "        Batch mode only. Layer name in hucs to use if multi-layer file is passed.\n",
    "    subset_hucs : str or list of str, optional\n",
    "        Batch mode only. File path to line delimited file, HUC string, or list of HUC strings to further subset hucs file for inundating.\n",
    "    num_workers : int, optional\n",
    "        Batch mode only. Number of workers to use in batch mode. Must be 1 or greater.\n",
    "    aggregate : bool, optional\n",
    "        Batch mode only. Aggregates output rasters to VRT mosaic files and merges polygons to single GPKG file. Currently not functional. Raises warning and sets to false. On to-do list.\n",
    "    inundation_raster : str, optional\n",
    "        Path to optional inundation raster output. Appends HUC number if ran in batch mode.\n",
    "    inundation_polygon : str, optional\n",
    "        Path to optional inundation vector output. Only accepts GPKG right now. Appends HUC number if ran in batch mode.\n",
    "    depths : str, optional\n",
    "        Path to optional depths raster output. Appends HUC number if ran in batch mode.\n",
    "    out_raster_profile : str or dictionary, optional\n",
    "        Override the default raster profile for outputs. See Rasterio profile documentation for more information.\n",
    "    out_vector_profile : str or dictionary\n",
    "        Override the default kwargs passed to fiona.Collection including crs, driver, and schema.\n",
    "    quiet : bool, optional\n",
    "        Quiet output.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    error_code : int\n",
    "        Zero for successful completion.\n",
    "\n",
    "    Raises\n",
    "    ------\n",
    "    TypeError\n",
    "        Wrong input data types\n",
    "    AssertionError\n",
    "        Wrong input data types\n",
    "\n",
    "    Warns\n",
    "    -----\n",
    "    warn\n",
    "        if aggregrate set to true, will revert to false.\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    - Specifying a subset of the domain in rem or catchments to inundate on is achieved by the HUCs file or the forecast file.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # check for num_workers\n",
    "    num_workers = int(num_workers)\n",
    "    assert num_workers >= 1, \"Number of workers should be 1 or greater\"\n",
    "    if (num_workers > 1) & (hucs is None):\n",
    "        raise AssertionError(\"Pass a HUCs file to batch process inundation mapping\")\n",
    "\n",
    "    # check that aggregate is only done for hucs mode\n",
    "    aggregate = bool(aggregate)\n",
    "    if aggregate:\n",
    "        warn(\"Aggregate feature currently not working. Setting to false for now.\")\n",
    "        aggregate = False\n",
    "    if hucs is None:\n",
    "        assert (not aggregate), \"Pass HUCs file if aggregation is desired\"\n",
    "\n",
    "    # bool quiet\n",
    "    quiet = bool(quiet)\n",
    "\n",
    "    # input rem\n",
    "    if isinstance(rem,str):\n",
    "        rem = rasterio.open(rem)\n",
    "    elif isinstance(rem,DatasetReader):\n",
    "        pass\n",
    "    else:\n",
    "        raise TypeError(\"Pass rasterio dataset or filepath for rem\")\n",
    "\n",
    "    # input catchments grid\n",
    "    if isinstance(catchments,str):\n",
    "        catchments = rasterio.open(catchments)\n",
    "    elif isinstance(catchments,DatasetReader):\n",
    "        pass\n",
    "    else:\n",
    "        raise TypeError(\"Pass rasterio dataset or filepath for catchments\")\n",
    "\n",
    "\n",
    "    # check for matching number of bands and single band only\n",
    "    assert rem.count == catchments.count == 1, \"REM and catchments rasters are required to be single band only\"\n",
    "\n",
    "    # check for matching raster sizes\n",
    "    assert (rem.width == catchments.width) & (rem.height == catchments.height), \"REM and catchments rasters required same shape\"\n",
    "\n",
    "    # check for matching projections\n",
    "    #assert rem.crs.to_proj4() == catchments.crs.to_proj4(), \"REM and Catchment rasters require same CRS definitions\"\n",
    "\n",
    "    # check for matching bounds\n",
    "    assert ( (rem.transform*(0,0)) == (catchments.transform*(0,0)) ) & ( (rem.transform* (rem.width,rem.height)) == (catchments.transform*(catchments.width,catchments.height)) ), \"REM and catchments rasters require same upper left and lower right extents\"\n",
    "\n",
    "    # open hucs\n",
    "    if hucs is None:\n",
    "        pass\n",
    "    elif isinstance(hucs,str):\n",
    "        hucs = fiona.open(hucs,'r',layer=hucs_layerName)\n",
    "    elif isinstance(hucs,fiona.Collection):\n",
    "        pass\n",
    "    else:\n",
    "        raise TypeError(\"Pass fiona collection or filepath for hucs\")\n",
    "\n",
    "    # check for matching projections\n",
    "    #assert to_string(hucs.crs) == rem.crs.to_proj4() == catchments.crs.to_proj4(), \"REM, Catchment, and HUCS CRS definitions must match\"\n",
    "\n",
    "    # catchment stages dictionary\n",
    "    if hydro_table is not None:\n",
    "        catchmentStagesDict,hucSet = __subset_hydroTable_to_forecast(hydro_table,forecast,subset_hucs)\n",
    "    else:\n",
    "        raise TypeError(\"Pass hydro table csv\")\n",
    "\n",
    "        \n",
    "    if catchmentStagesDict is not None:\n",
    "        if src_table is not None:\n",
    "            create_src_subset_csv(hydro_table,catchmentStagesDict,src_table)\n",
    "\n",
    "        # make windows generator\n",
    "        window_gen = __make_windows_generator(rem, catchments, catchment_poly,\n",
    "                                              mask_type, catchmentStagesDict, inundation_raster,\n",
    "                                              inundation_polygon, depths, out_raster_profile,\n",
    "                                              out_vector_profile, quiet, \n",
    "                                              hucs = hucs, hucSet = hucSet)\n",
    "\n",
    "        # start up thread pool\n",
    "        executor = ThreadPoolExecutor(max_workers=num_workers)\n",
    "\n",
    "        # submit jobs\n",
    "        results = {executor.submit(__inundate_in_huc,*wg) : wg[6] for wg in window_gen}\n",
    "\n",
    "        inundation_rasters = [] ; depth_rasters = [] ; inundation_polys = []\n",
    "        for future in as_completed(results):\n",
    "            try:\n",
    "                future.result()\n",
    "            except Exception as exc:\n",
    "                __vprint(\"Exception {} for {}\".format(exc,results[future]),not quiet)\n",
    "            else:\n",
    "                if results[future] is not None:\n",
    "                    __vprint(\"... {} complete\".format(results[future]),not quiet)\n",
    "                else:\n",
    "                    __vprint(\"... complete\",not quiet)\n",
    "\n",
    "                inundation_rasters += [future.result()[0]]\n",
    "                depth_rasters += [future.result()[1]]\n",
    "                inundation_polys += [future.result()[2]]\n",
    "\n",
    "        # power down pool\n",
    "        executor.shutdown(wait=True)\n",
    "\n",
    "    # close datasets\n",
    "    rem.close()\n",
    "    catchments.close()\n",
    "\n",
    "    return(inundation_rasters,depth_rasters,inundation_polys)\n",
    "\n",
    "\n",
    "def __inundate_in_huc(rem_array,catchments_array,crs,window_transform,rem_profile,catchments_profile,hucCode,\n",
    "                      catchmentStagesDict,depths,inundation_raster,inundation_polygon,\n",
    "                      out_raster_profile,out_vector_profile,quiet):\n",
    "\n",
    "    # verbose print\n",
    "    if hucCode is not None:\n",
    "        __vprint(\"Inundating {} ...\".format(hucCode),not quiet)\n",
    "\n",
    "    # save desired profiles for outputs\n",
    "    depths_profile = rem_profile\n",
    "    inundation_profile = catchments_profile\n",
    "\n",
    "    # update output profiles from inputs\n",
    "    if isinstance(out_raster_profile,dict):\n",
    "        depths_profile.update(**out_raster_profile)\n",
    "        inundation_profile.update(**out_raster_profile)\n",
    "    elif out_raster_profile is None:\n",
    "        depths_profile.update(driver= 'GTiff', blockxsize=256, blockysize=256, tiled=True, compress='lzw')\n",
    "        inundation_profile.update(driver= 'GTiff',blockxsize=256, blockysize=256, tiled=True, compress='lzw')\n",
    "    else:\n",
    "        raise TypeError(\"Pass dictionary for output raster profiles\")\n",
    "\n",
    "    # update profiles with width and heights from array sizes\n",
    "    depths_profile.update(height=rem_array.shape[0],width=rem_array.shape[1])\n",
    "    inundation_profile.update(height=catchments_array.shape[0],width=catchments_array.shape[1])\n",
    "\n",
    "    # update transforms of outputs with window transform\n",
    "    depths_profile.update(transform=window_transform)\n",
    "    inundation_profile.update(transform=window_transform)\n",
    "    # open output depths\n",
    "    if isinstance(depths,str):\n",
    "        depths = __append_huc_code_to_file_name(depths,hucCode)\n",
    "        depths = rasterio.open(depths, \"w\", **depths_profile)\n",
    "    elif isinstance(depths,DatasetWriter):\n",
    "        pass\n",
    "    elif depths is None:\n",
    "        pass\n",
    "    else:\n",
    "        raise TypeError(\"Pass rasterio dataset, filepath for output depths, or None.\")\n",
    "\n",
    "    # open output inundation raster\n",
    "    if isinstance(inundation_raster,str):\n",
    "        inundation_raster = __append_huc_code_to_file_name(inundation_raster,hucCode)\n",
    "        inundation_raster = rasterio.open(inundation_raster,\"w\",**inundation_profile)\n",
    "    elif isinstance(inundation_raster,DatasetWriter):\n",
    "        pass\n",
    "    elif inundation_raster is None:\n",
    "        pass\n",
    "    else:\n",
    "        raise TypeError(\"Pass rasterio dataset, filepath for output inundation raster, or None.\")\n",
    "\n",
    "    # prepare output inundation polygons schema\n",
    "    if inundation_polygon is not None:\n",
    "        if out_vector_profile is None:\n",
    "            out_vector_profile = {'crs' : crs , 'driver' : 'GPKG'}\n",
    "\n",
    "        out_vector_profile['schema'] = {\n",
    "                                         'geometry' : 'Polygon',\n",
    "                                         'properties' : OrderedDict([('HydroID' , 'int')])\n",
    "                                       }\n",
    "\n",
    "        # open output inundation polygons\n",
    "        if isinstance(inundation_polygon,str):\n",
    "            inundation_polygon = __append_huc_code_to_file_name(inundation_polygon,hucCode)\n",
    "            inundation_polygon = fiona.open(inundation_polygon,'w',**out_vector_profile)\n",
    "        elif isinstance(inundation_polygon,fiona.Collection):\n",
    "            pass\n",
    "        else:\n",
    "            raise TypeError(\"Pass fiona collection or file path as inundation_polygon\")\n",
    "\n",
    "    # save desired array shape\n",
    "    desired_shape = rem_array.shape\n",
    "\n",
    "    # flatten\n",
    "    rem_array = rem_array.ravel()\n",
    "    catchments_array = catchments_array.ravel()\n",
    "\n",
    "    # create flat outputs\n",
    "    depths_array = rem_array.copy()\n",
    "    inundation_array = catchments_array.copy()\n",
    "\n",
    "    # reset output values\n",
    "    depths_array[depths_array != depths_profile['nodata']] = 0\n",
    "    inundation_array[inundation_array != inundation_profile['nodata']] = inundation_array[inundation_array != inundation_profile['nodata']] * -1\n",
    "\n",
    "    # make output arrays\n",
    "    inundation_array,depths_array = __go_fast_mapping(rem_array,catchments_array,catchmentStagesDict,inundation_array,depths_array)\n",
    "\n",
    "    # reshape output arrays\n",
    "    inundation_array = inundation_array.reshape(desired_shape)\n",
    "    depths_array = depths_array.reshape(desired_shape)\n",
    "\n",
    "    # write out inundation and depth rasters\n",
    "    if isinstance(inundation_raster,DatasetWriter):\n",
    "        inundation_raster.write(inundation_array,indexes=1)\n",
    "    if isinstance(depths,DatasetWriter):\n",
    "        depths.write(depths_array,indexes=1)\n",
    "\n",
    "    # polygonize inundation\n",
    "    if isinstance(inundation_polygon,fiona.Collection):\n",
    "\n",
    "        # make generator for inundation polygons\n",
    "        inundation_polygon_generator = shapes(inundation_array,mask=inundation_array>0,connectivity=8,transform=window_transform)\n",
    "        \n",
    "        # generate records\n",
    "        records = []\n",
    "        for i,(g,h) in enumerate(inundation_polygon_generator):\n",
    "            record = dict()\n",
    "            record['geometry'] = g\n",
    "            record['properties'] = {'HydroID' : int(h)}\n",
    "            records += [record]\n",
    "        \n",
    "        # write out\n",
    "        inundation_polygon.writerecords(records)\n",
    "\n",
    "    if isinstance(depths,DatasetWriter): depths.close()\n",
    "    if isinstance(inundation_raster,DatasetWriter): inundation_raster.close()\n",
    "    if isinstance(inundation_polygon,fiona.Collection): inundation_polygon.close()\n",
    "    #if isinstance(hucs,fiona.Collection): inundation_polygon.close()\n",
    "\n",
    "    # return file names of outputs for aggregation. Handle Nones\n",
    "    try:\n",
    "        ir_name = inundation_raster.name\n",
    "    except AttributeError:\n",
    "        ir_name = None\n",
    "\n",
    "    try:\n",
    "        d_name = depths.name\n",
    "    except AttributeError:\n",
    "        d_name = None\n",
    "\n",
    "    try:\n",
    "        ip_name = inundation_polygon.path\n",
    "    except AttributeError:\n",
    "        ip_name = None\n",
    "    \n",
    "    #print(ir_name)\n",
    "    #yield(ir_name,d_name,ip_name)\n",
    "    \n",
    "    if isinstance(depths,DatasetWriter): depths.close()\n",
    "    if isinstance(inundation_raster,DatasetWriter): inundation_raster.close()\n",
    "    if isinstance(inundation_polygon,fiona.Collection): inundation_polygon.close()\n",
    "\n",
    "    return(ir_name,d_name,ip_name)\n",
    "\n",
    "\n",
    "@njit\n",
    "def __go_fast_mapping(rem,catchments,catchmentStagesDict,inundation,depths):\n",
    "\n",
    "    for i,(r,cm) in enumerate(zip(rem,catchments)):\n",
    "        if cm in catchmentStagesDict:\n",
    "\n",
    "            depth = catchmentStagesDict[cm] - r\n",
    "            depths[i] = max(depth,0) # set negative depths to 0\n",
    "\n",
    "            if depths[i] > 0: # set positive depths to positive\n",
    "                inundation[i] *= -1\n",
    "            #else: # set positive depths to value of positive catchment value\n",
    "                #inundation[i] = cm\n",
    "\n",
    "    return(inundation,depths)\n",
    "\n",
    "\n",
    "def __make_windows_generator(rem, \n",
    "                             catchments,\n",
    "                             catchment_poly,\n",
    "                             mask_type,\n",
    "                             catchmentStagesDict,\n",
    "                             inundation_raster,\n",
    "                             inundation_polygon,\n",
    "                             depths,\n",
    "                             out_raster_profile,\n",
    "                             out_vector_profile,\n",
    "                             quiet,\n",
    "                             hucs = None,\n",
    "                             hucSet = None):\n",
    "\n",
    "    \n",
    "    if hucs is not None:\n",
    "\n",
    "        # get attribute name for HUC column\n",
    "        for huc in hucs:\n",
    "            for hucColName in huc['properties'].keys():\n",
    "                if 'HUC' in hucColName:\n",
    "                    hucSize = int(hucColName[-1])\n",
    "                    break\n",
    "            break\n",
    "\n",
    "        # make windows\n",
    "        for huc in hucs:\n",
    "\n",
    "            # returns hucCode if current huc is in hucSet (at least starts with)\n",
    "            def __return_huc_in_hucSet(hucCode,hucSet):\n",
    "\n",
    "                for hs in hucSet:\n",
    "                    if hs.startswith(hucCode):\n",
    "                        return(hucCode)\n",
    "\n",
    "                return(None)\n",
    "\n",
    "            if  __return_huc_in_hucSet(huc['properties'][hucColName],hucSet) is None:\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                if mask_type == \"huc\":\n",
    "                    #window = geometry_window(rem,shape(huc['geometry']))\n",
    "                    rem_array,window_transform = mask(rem,[shape(huc['geometry'])],crop=True,indexes=1)\n",
    "                    catchments_array,_ = mask(catchments,[shape(huc['geometry'])],crop=True,indexes=1)\n",
    "                elif mask_type == \"filter\":\n",
    "\n",
    "                    # input catchments polygon\n",
    "                    if isinstance(catchment_poly,str):\n",
    "                        catchment_poly=gpd.read_file(catchment_poly)\n",
    "                    elif isinstance(catchment_poly,DatasetReader):\n",
    "                        pass\n",
    "                    else:\n",
    "                        raise TypeError(\"Pass geopandas dataset or filepath for catchment polygons\")\n",
    "\n",
    "                    fossid = huc['properties']['fossid']\n",
    "                    if catchment_poly.comid.dtype != 'str': catchment_poly.comid = catchment_poly.comid.astype(str)\n",
    "                    catchment_poly=catchment_poly[catchment_poly.comid.str.startswith(fossid)]\n",
    "\n",
    "                    rem_array,window_transform = mask(rem,catchment_poly['geometry'],crop=True,indexes=1)\n",
    "                    catchments_array,_ = mask(catchments,catchment_poly['geometry'],crop=True,indexes=1)\n",
    "                    del catchment_poly\n",
    "                elif mask_type is None:\n",
    "                    pass\n",
    "                else:\n",
    "                    print (\"invalid mask type. Options are 'huc' or 'filter'\")\n",
    "            except ValueError: # shape doesn't overlap raster\n",
    "                continue # skip to next HUC\n",
    "\n",
    "            hucCode = huc['properties'][hucColName]\n",
    "\n",
    "            yield (rem_array, catchments_array, rem.crs.wkt,\n",
    "                   window_transform, rem.profile, catchments.profile, hucCode,\n",
    "                   catchmentStagesDict, depths, inundation_raster,\n",
    "                   inundation_polygon, out_raster_profile, out_vector_profile, quiet)\n",
    "\n",
    "    else:\n",
    "        hucCode = None\n",
    "       #window = Window(col_off=0,row_off=0,width=rem.width,height=rem.height)\n",
    "\n",
    "        yield (rem.read(1),catchments.read(1),rem.crs.wkt,\n",
    "               rem.transform,rem.profile,catchments.profile,hucCode,\n",
    "               catchmentStagesDict,depths,inundation_raster,\n",
    "               inundation_polygon,out_raster_profile,out_vector_profile,quiet)\n",
    "\n",
    "\n",
    "def __append_huc_code_to_file_name(fileName,hucCode):\n",
    "\n",
    "    if hucCode is None:\n",
    "        return(fileName)\n",
    "\n",
    "    base_file_path,extension = splitext(fileName)\n",
    "\n",
    "    return(\"{}_{}{}\".format(base_file_path,hucCode,extension))\n",
    "\n",
    "\n",
    "def __subset_hydroTable_to_forecast(hydroTable,forecast,subset_hucs=None):\n",
    "\n",
    "    if isinstance(hydroTable,str):\n",
    "        hydroTable = pd.read_csv(\n",
    "                                 hydroTable,\n",
    "                                 dtype={'CatchId':str,'Stage':float,\n",
    "                                         'Discharge (m3s-1)':float}\n",
    "                                )\n",
    "        hydroTable['feature_id']=hydroTable['CatchId']\n",
    "        hydroTable['HUC']='010802'\n",
    "        huc_error = hydroTable.HUC.unique()\n",
    "        hydroTable.set_index(['HUC','feature_id','CatchId'],inplace=True)\n",
    "\n",
    "    elif isinstance(hydroTable,pd.DataFrame):\n",
    "        pass #consider checking for correct dtypes, indices, and columns\n",
    "    else:\n",
    "        raise TypeError(\"Pass path to hydro-table csv or Pandas DataFrame\")\n",
    "\n",
    "\n",
    "    # raises error if hydroTable is empty due to all segments being lakes\n",
    "    if hydroTable.empty:\n",
    "        raise hydroTableHasOnlyLakes(\"All stream segments in HUC are within lake boundaries.\")\n",
    "\n",
    "\n",
    "    if isinstance(forecast,str):\n",
    "\n",
    "        try:\n",
    "            forecast = pd.read_csv(\n",
    "                                   forecast,\n",
    "                                   dtype={'feature_id' : str , 'discharge' : float}\n",
    "                                  )\n",
    "            forecast.set_index('feature_id',inplace=True)\n",
    "        except UnicodeDecodeError:\n",
    "            forecast = read_nwm_forecast_file(forecast)\n",
    "    \n",
    "    elif isinstance(forecast,pd.DataFrame):\n",
    "        pass # consider checking for dtypes, indices, and columns\n",
    "    else:\n",
    "        raise TypeError(\"Pass path to forecast file csv or Pandas DataFrame\")\n",
    "\n",
    "    # susbset hucs if passed\n",
    "    if subset_hucs is not None:\n",
    "        if isinstance(subset_hucs,list):\n",
    "            if len(subset_hucs) == 1:\n",
    "                try:\n",
    "                    subset_hucs = open(subset_hucs[0]).read().split('\\n')\n",
    "                except FileNotFoundError:\n",
    "                    pass\n",
    "        elif isinstance(subset_hucs,str):\n",
    "                try:\n",
    "                    subset_hucs = open(subset_hucs).read().split('\\n')\n",
    "                except FileNotFoundError:\n",
    "                    subset_hucs = [subset_hucs]\n",
    "\n",
    "    if not hydroTable.empty:\n",
    "\n",
    "        if isinstance(forecast,str):\n",
    "            forecast = pd.read_csv(\n",
    "                                   forecast,\n",
    "                                   dtype={'feature_id' : str , 'discharge' : float}\n",
    "                                  )\n",
    "            forecast.set_index('feature_id',inplace=True)\n",
    "        elif isinstance(forecast,pd.DataFrame):\n",
    "            pass # consider checking for dtypes, indices, and columns\n",
    "        else:\n",
    "            raise TypeError(\"Pass path to forecast file csv or Pandas DataFrame\")\n",
    "\n",
    "        # susbset hucs if passed\n",
    "        if subset_hucs is not None:\n",
    "            if isinstance(subset_hucs,list):\n",
    "                if len(subset_hucs) == 1:\n",
    "                    try:\n",
    "                        subset_hucs = open(subset_hucs[0]).read().split('\\n')\n",
    "                    except FileNotFoundError:\n",
    "                        pass\n",
    "            elif isinstance(subset_hucs,str):\n",
    "                    try:\n",
    "                        subset_hucs = open(subset_hucs).read().split('\\n')\n",
    "                    except FileNotFoundError:\n",
    "                        subset_hucs = [subset_hucs]\n",
    "\n",
    "            # subsets HUCS\n",
    "            subset_hucs_orig = subset_hucs.copy() ; subset_hucs = []\n",
    "            for huc in np.unique(hydroTable.index.get_level_values('HUC')):\n",
    "                for sh in subset_hucs_orig:\n",
    "                    if huc.startswith(sh):\n",
    "                        subset_hucs += [huc]\n",
    "\n",
    "            hydroTable = hydroTable[np.in1d(hydroTable.index.get_level_values('HUC'), subset_hucs)]\n",
    "\n",
    "    # join tables\n",
    "    try:\n",
    "        hydroTable = hydroTable.join(forecast,on=['feature_id'],how='inner')\n",
    "    except AttributeError:\n",
    "        #print(\"FORECAST ERROR\")\n",
    "        raise NoForecastFound(\"No forecast value found for the passed feature_ids in the Hydro-Table\")\n",
    "\n",
    "    else:\n",
    "\n",
    "        # initialize dictionary\n",
    "        catchmentStagesDict = typed.Dict.empty(types.int32,types.float64)\n",
    "\n",
    "        # interpolate stages\n",
    "        for hid,sub_table in hydroTable.groupby(level='CatchId'):\n",
    "\n",
    "            interpolated_stage = np.interp(sub_table.loc[:,'discharge'].unique(),sub_table.loc[:,'Discharge (m3s-1)'],sub_table.loc[:,'Stage'])\n",
    "\n",
    "            # add this interpolated stage to catchment stages dict\n",
    "            h = round(interpolated_stage[0],4)\n",
    "\n",
    "            hid = types.int32(hid) ; h = types.float32(h)\n",
    "            catchmentStagesDict[hid] = h\n",
    "\n",
    "        # huc set\n",
    "        hucSet = [str(i) for i in hydroTable.index.get_level_values('HUC').unique().to_list()]\n",
    "\n",
    "        return(catchmentStagesDict,hucSet)\n",
    "\n",
    "\n",
    "def read_nwm_forecast_file(forecast_file,rename_headers=True):\n",
    "        \n",
    "    \"\"\" Reads NWM netcdf comp files and converts to forecast data frame \"\"\"\n",
    "\n",
    "    flows_nc = xr.open_dataset(forecast_file,decode_cf='feature_id',engine='netcdf4')\n",
    "    \n",
    "    flows_df = flows_nc.to_dataframe()\n",
    "    flows_df.reset_index(inplace=True)\n",
    "    \n",
    "    flows_df = flows_df[['streamflow','feature_id']]\n",
    "    \n",
    "    if rename_headers:\n",
    "        flows_df = flows_df.rename(columns={\"streamflow\": \"discharge\"})\n",
    "\n",
    "    convert_dict = {'feature_id': str,'discharge': float}\n",
    "    flows_df = flows_df.astype(convert_dict)\n",
    "    \n",
    "    flows_df.set_index('feature_id',inplace=True,drop=True)\n",
    "\n",
    "    flows_df.dropna(inplace=True)\n",
    "\n",
    "    return(flows_df)\n",
    "\n",
    "\n",
    "def __vprint(message,verbose):\n",
    "    if verbose:\n",
    "        print(message)\n",
    "\n",
    "def create_src_subset_csv(hydro_table,catchmentStagesDict,src_table):\n",
    "    src_df = pd.DataFrame.from_dict(catchmentStagesDict, orient='index')\n",
    "    src_df.reset_index(inplace=True)\n",
    "    src_df.columns = ['CatchId','stage_inund']\n",
    "    df_htable = pd.read_csv(hydro_table,dtype={'CatchId': int})\n",
    "    df_htable = df_htable.merge(src_df,how='left',on='CatchId')\n",
    "    df_htable['find_match'] = (df_htable['Stage'] - df_htable['stage_inund']).abs()\n",
    "    df_htable = df_htable.loc[df_htable.groupby('CatchId')['find_match'].idxmin()].reset_index(drop=True)\n",
    "    df_htable.to_csv(src_table,index=False)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    # parse arguments\n",
    "    parser = argparse.ArgumentParser(description='Rapid inundation mapping for FOSS FIM. Operates in single-HUC and batch modes.')\n",
    "    parser.add_argument('-r', '--rem', \n",
    "                        help='REM raster at job level or mosaic vrt. Must match catchments CRS.', \n",
    "                        required=True)\n",
    "    parser.add_argument('-c', '--catchments',\n",
    "                        help='Catchments raster at job level or mosaic VRT. Must match rem CRS.',\n",
    "                        required=True)\n",
    "    parser.add_argument('-b', '--catchment-poly', help='catchment_vector',\n",
    "                        required=True)\n",
    "    parser.add_argument('-t', '--hydro-table', help='Hydro-table in csv file format', \n",
    "                        required=True)\n",
    "    parser.add_argument('-f', '--forecast', help='Forecast discharges in CMS as CSV file',\n",
    "                        required=True)\n",
    "    parser.add_argument('-u', '--hucs',\n",
    "                        help='Batch mode only: HUCs file to process at. Must match CRS of input rasters',required=False, default=None)\n",
    "    parser.add_argument('-l', '--hucs-layerName', help='Batch mode only. Layer name in HUCs file to use',\n",
    "                        required=False, default=None)\n",
    "    parser.add_argument('-j', '--num-workers',help='Batch mode only. Number of concurrent processes',\n",
    "                        required=False, default=1, type=int)\n",
    "    parser.add_argument('-s', '--subset-hucs', help = \"\"\"Batch mode only. HUC code, \n",
    "            series of HUC codes (no quotes required), or line delimited of HUCs to run within \n",
    "            the hucs file that is passed\"\"\",\n",
    "                        required=False, default=None, nargs='+')\n",
    "    parser.add_argument('-m', '--mask-type',\n",
    "                        help='Specify huc (FIM < 3) or filter (FIM >= 3) masking method',\n",
    "                        required=False, default=\"huc\")\n",
    "    parser.add_argument('-a', '--aggregate',\n",
    "                        help=\"\"\"Batch mode only. Aggregate outputs to VRT files.\n",
    "                        Currently, raises warning and sets to false if used.\"\"\",\n",
    "                        required=False, action='store_true')\n",
    "    parser.add_argument('-i', '--inundation-raster',\n",
    "                        help=\"\"\"Inundation Raster output. Only writes if designated.\n",
    "                        Appends HUC code in batch mode.\"\"\",\n",
    "                        required=False, default=None)\n",
    "    parser.add_argument('-p', '--inundation-polygon',\n",
    "                        help=\"\"\"Inundation polygon output. Only writes if designated. \n",
    "                        Appends HUC code in batch mode.\"\"\",\n",
    "                        required=False, default=None)\n",
    "    parser.add_argument('-d', '--depths',\n",
    "                        help=\"\"\"Depths raster output. Only writes if designated.\n",
    "                        Appends HUC code in batch mode.\"\"\",\n",
    "                        required=False, default=None)\n",
    "    parser.add_argument('-n', '--src-table',\n",
    "                        help=\"\"\"Output table with the SRC lookup/interpolation.\n",
    "                        Only writes if designated. Appends HUC code in batch mode.\"\"\",\n",
    "                        required=False, default=None)\n",
    "    parser.add_argument('-q','--quiet', help='Quiet terminal output',\n",
    "                        required=False, default=False, action='store_true')\n",
    "\n",
    "    # extract to dictionary\n",
    "    args = vars(parser.parse_args())\n",
    "\n",
    "    # call function\n",
    "    inundate(**args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d6e747fa-a5ef-45a1-a96e-03baabc57f0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class hydroTableHasOnlyLakes(Exception): \n",
    "    \"\"\" Raised when a Hydro-Table only has lakes \"\"\"\n",
    "    pass\n",
    "\n",
    "\n",
    "class NoForecastFound(Exception):\n",
    "    \"\"\" Raised when no forecast is available for a given Hydro-Table \"\"\"\n",
    "    pass\n",
    "\n",
    "class hydroTableHasOnlyLakes(Exception): \n",
    "    \"\"\" Raised when a Hydro-Table only has lakes \"\"\"\n",
    "    pass\n",
    "\n",
    "\n",
    "class NoForecastFound(Exception):\n",
    "    \"\"\" Raised when no forecast is available for a given Hydro-Table \"\"\"\n",
    "    pass\n",
    "\n",
    "def inundate(rem, catchments, catchment_poly, hydro_table, forecast,\n",
    "             mask_type, huc_6_id, hucs = None, hucs_layerName = None,\n",
    "             subset_hucs = None, num_workers = 1, aggregate = False, \n",
    "             inundation_raster = None, inundation_polygon = None,\n",
    "             depths = None, out_raster_profile = None, out_vector_profile = None,\n",
    "             src_table = None, quiet = False):\n",
    "    \"\"\"\n",
    "\n",
    "    Run inundation on FIM >=3.0 outputs at job-level scale or aggregated scale\n",
    "\n",
    "    Generate depths raster, inundation raster, and inundation polygon from FIM >=3.0 outputs. Can use the FIM 3.0 outputs at native HUC level or the aggregated products. Be sure to pass a HUCs file to process in batch mode if passing aggregated products.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    rem : str or rasterio.DatasetReader\n",
    "        File path to or rasterio dataset reader of Relative Elevation Model raster. Must have the same CRS as catchments raster.\n",
    "    catchments : str or rasterio.DatasetReader\n",
    "        File path to or rasterio dataset reader of Catchments raster. Must have the same CRS as REM raster\n",
    "    hydro_table : str or pandas.DataFrame\n",
    "        File path to hydro-table csv or Pandas DataFrame object with correct indices and columns.\n",
    "    forecast : str or pandas.DataFrame\n",
    "        File path to forecast csv or Pandas DataFrame with correct column names.\n",
    "    hucs : str or fiona.Collection, optional\n",
    "        Batch mode only. File path or fiona collection of vector polygons in HUC 4,6,or 8's to inundate on. Must have an attribute named as either \"HUC4\",\"HUC6\", or \"HUC8\" with the associated values.\n",
    "    hucs_layerName : str, optional\n",
    "        Batch mode only. Layer name in hucs to use if multi-layer file is passed.\n",
    "    subset_hucs : str or list of str, optional\n",
    "        Batch mode only. File path to line delimited file, HUC string, or list of HUC strings to further subset hucs file for inundating.\n",
    "    num_workers : int, optional\n",
    "        Batch mode only. Number of workers to use in batch mode. Must be 1 or greater.\n",
    "    aggregate : bool, optional\n",
    "        Batch mode only. Aggregates output rasters to VRT mosaic files and merges polygons to single GPKG file. Currently not functional. Raises warning and sets to false. On to-do list.\n",
    "    inundation_raster : str, optional\n",
    "        Path to optional inundation raster output. Appends HUC number if ran in batch mode.\n",
    "    inundation_polygon : str, optional\n",
    "        Path to optional inundation vector output. Only accepts GPKG right now. Appends HUC number if ran in batch mode.\n",
    "    depths : str, optional\n",
    "        Path to optional depths raster output. Appends HUC number if ran in batch mode.\n",
    "    out_raster_profile : str or dictionary, optional\n",
    "        Override the default raster profile for outputs. See Rasterio profile documentation for more information.\n",
    "    out_vector_profile : str or dictionary\n",
    "        Override the default kwargs passed to fiona.Collection including crs, driver, and schema.\n",
    "    quiet : bool, optional\n",
    "        Quiet output.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    error_code : int\n",
    "        Zero for successful completion.\n",
    "\n",
    "    Raises\n",
    "    ------\n",
    "    TypeError\n",
    "        Wrong input data types\n",
    "    AssertionError\n",
    "        Wrong input data types\n",
    "\n",
    "    Warns\n",
    "    -----\n",
    "    warn\n",
    "        if aggregrate set to true, will revert to false.\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    - Specifying a subset of the domain in rem or catchments to inundate on is achieved by the HUCs file or the forecast file.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # check for num_workers\n",
    "    num_workers = int(num_workers)\n",
    "    assert num_workers >= 1, \"Number of workers should be 1 or greater\"\n",
    "    if (num_workers > 1) & (hucs is None):\n",
    "        raise AssertionError(\"Pass a HUCs file to batch process inundation mapping\")\n",
    "\n",
    "    # check that aggregate is only done for hucs mode\n",
    "    aggregate = bool(aggregate)\n",
    "    if aggregate:\n",
    "        warn(\"Aggregate feature currently not working. Setting to false for now.\")\n",
    "        aggregate = False\n",
    "    if hucs is None:\n",
    "        assert (not aggregate), \"Pass HUCs file if aggregation is desired\"\n",
    "\n",
    "    # bool quiet\n",
    "    quiet = bool(quiet)\n",
    "\n",
    "    # input rem\n",
    "    if isinstance(rem,str):\n",
    "        rem = rasterio.open(rem)\n",
    "    elif isinstance(rem,DatasetReader):\n",
    "        pass\n",
    "    else:\n",
    "        raise TypeError(\"Pass rasterio dataset or filepath for rem\")\n",
    "\n",
    "    # input catchments grid\n",
    "    if isinstance(catchments,str):\n",
    "        catchments = rasterio.open(catchments)\n",
    "    elif isinstance(catchments,DatasetReader):\n",
    "        pass\n",
    "    else:\n",
    "        raise TypeError(\"Pass rasterio dataset or filepath for catchments\")\n",
    "\n",
    "\n",
    "    # check for matching number of bands and single band only\n",
    "    assert rem.count == catchments.count == 1, \"REM and catchments rasters are required to be single band only\"\n",
    "\n",
    "    # check for matching raster sizes\n",
    "    assert (rem.width == catchments.width) & (rem.height == catchments.height), \"REM and catchments rasters required same shape\"\n",
    "\n",
    "    # check for matching projections\n",
    "    #assert rem.crs.to_proj4() == catchments.crs.to_proj4(), \"REM and Catchment rasters require same CRS definitions\"\n",
    "\n",
    "    # check for matching bounds\n",
    "    #assert ( (rem.transform*(0,0)) == (catchments.transform*(0,0)) ) & ( (rem.transform* (rem.width,rem.height)) == (catchments.transform*(catchments.width,catchments.height)) ), \"REM and catchments rasters require same upper left and lower right extents\"\n",
    "\n",
    "    # open hucs\n",
    "    if hucs is None:\n",
    "        pass\n",
    "    elif isinstance(hucs,str):\n",
    "        hucs = fiona.open(hucs,'r',layer=hucs_layerName)\n",
    "    elif isinstance(hucs,fiona.Collection):\n",
    "        pass\n",
    "    else:\n",
    "        raise TypeError(\"Pass fiona collection or filepath for hucs\")\n",
    "\n",
    "    # check for matching projections\n",
    "    #assert to_string(hucs.crs) == rem.crs.to_proj4() == catchments.crs.to_proj4(), \"REM, Catchment, and HUCS CRS definitions must match\"\n",
    "\n",
    "    # catchment stages dictionary\n",
    "    if hydro_table is not None:\n",
    "        catchmentStagesDict,hucSet = __subset_hydroTable_to_forecast(huc_6_id,hydro_table,forecast,subset_hucs)\n",
    "    else:\n",
    "        raise TypeError(\"Pass hydro table csv\")\n",
    "\n",
    "        \n",
    "    if catchmentStagesDict is not None:\n",
    "        if src_table is not None:\n",
    "            create_src_subset_csv(hydro_table,catchmentStagesDict,src_table)\n",
    "\n",
    "        # make windows generator\n",
    "        window_gen = __make_windows_generator(rem, catchments, catchment_poly,\n",
    "                                              mask_type, catchmentStagesDict, inundation_raster,\n",
    "                                              inundation_polygon, depths, out_raster_profile,\n",
    "                                              out_vector_profile, quiet, \n",
    "                                              hucs = hucs, hucSet = hucSet)\n",
    "\n",
    "        # start up thread pool\n",
    "        executor = ThreadPoolExecutor(max_workers=num_workers)\n",
    "\n",
    "        # submit jobs\n",
    "        results = {executor.submit(__inundate_in_huc,*wg) : wg[6] for wg in window_gen}\n",
    "\n",
    "        inundation_rasters = [] ; depth_rasters = [] ; inundation_polys = []\n",
    "        for future in as_completed(results):\n",
    "            try:\n",
    "                future.result()\n",
    "            except Exception as exc:\n",
    "                __vprint(\"Exception {} for {}\".format(exc,results[future]),not quiet)\n",
    "            else:\n",
    "                if results[future] is not None:\n",
    "                    __vprint(\"... {} complete\".format(results[future]),not quiet)\n",
    "                else:\n",
    "                    __vprint(\"... complete\",not quiet)\n",
    "\n",
    "                inundation_rasters += [future.result()[0]]\n",
    "                depth_rasters += [future.result()[1]]\n",
    "                inundation_polys += [future.result()[2]]\n",
    "\n",
    "        # power down pool\n",
    "        executor.shutdown(wait=True)\n",
    "\n",
    "    # close datasets\n",
    "    rem.close()\n",
    "    catchments.close()\n",
    "\n",
    "    return(inundation_rasters,depth_rasters,inundation_polys)\n",
    "\n",
    "\n",
    "def __inundate_in_huc(rem_array,catchments_array,crs,window_transform,rem_profile,catchments_profile,hucCode,\n",
    "                      catchmentStagesDict,depths,inundation_raster,inundation_polygon,\n",
    "                      out_raster_profile,out_vector_profile,quiet):\n",
    "\n",
    "    # verbose print\n",
    "    if hucCode is not None:\n",
    "        __vprint(\"Inundating {} ...\".format(hucCode),not quiet)\n",
    "\n",
    "    # save desired profiles for outputs\n",
    "    depths_profile = rem_profile\n",
    "    inundation_profile = catchments_profile\n",
    "\n",
    "    # update output profiles from inputs\n",
    "    if isinstance(out_raster_profile,dict):\n",
    "        depths_profile.update(**out_raster_profile)\n",
    "        inundation_profile.update(**out_raster_profile)\n",
    "    elif out_raster_profile is None:\n",
    "        depths_profile.update(driver= 'GTiff', blockxsize=256, blockysize=256, tiled=True, compress='lzw')\n",
    "        inundation_profile.update(driver= 'GTiff',blockxsize=256, blockysize=256, tiled=True, compress='lzw')\n",
    "    else:\n",
    "        raise TypeError(\"Pass dictionary for output raster profiles\")\n",
    "\n",
    "    # update profiles with width and heights from array sizes\n",
    "    depths_profile.update(height=rem_array.shape[0],width=rem_array.shape[1])\n",
    "    inundation_profile.update(height=catchments_array.shape[0],width=catchments_array.shape[1])\n",
    "\n",
    "    # update transforms of outputs with window transform\n",
    "    depths_profile.update(transform=window_transform)\n",
    "    inundation_profile.update(transform=window_transform)\n",
    "    # open output depths\n",
    "    if isinstance(depths,str):\n",
    "        depths = __append_huc_code_to_file_name(depths,hucCode)\n",
    "        depths = rasterio.open(depths, \"w\", **depths_profile)\n",
    "    elif isinstance(depths,DatasetWriter):\n",
    "        pass\n",
    "    elif depths is None:\n",
    "        pass\n",
    "    else:\n",
    "        raise TypeError(\"Pass rasterio dataset, filepath for output depths, or None.\")\n",
    "\n",
    "    # open output inundation raster\n",
    "    if isinstance(inundation_raster,str):\n",
    "        inundation_raster = __append_huc_code_to_file_name(inundation_raster,hucCode)\n",
    "        inundation_raster = rasterio.open(inundation_raster,\"w\",**inundation_profile)\n",
    "    elif isinstance(inundation_raster,DatasetWriter):\n",
    "        pass\n",
    "    elif inundation_raster is None:\n",
    "        pass\n",
    "    else:\n",
    "        raise TypeError(\"Pass rasterio dataset, filepath for output inundation raster, or None.\")\n",
    "\n",
    "    # prepare output inundation polygons schema\n",
    "    if inundation_polygon is not None:\n",
    "        if out_vector_profile is None:\n",
    "            out_vector_profile = {'crs' : crs , 'driver' : 'GPKG'}\n",
    "\n",
    "        out_vector_profile['schema'] = {\n",
    "                                         'geometry' : 'Polygon',\n",
    "                                         'properties' : OrderedDict([('HydroID' , 'int')])\n",
    "                                       }\n",
    "\n",
    "        # open output inundation polygons\n",
    "        if isinstance(inundation_polygon,str):\n",
    "            inundation_polygon = __append_huc_code_to_file_name(inundation_polygon,hucCode)\n",
    "            inundation_polygon = fiona.open(inundation_polygon,'w',**out_vector_profile)\n",
    "        elif isinstance(inundation_polygon,fiona.Collection):\n",
    "            pass\n",
    "        else:\n",
    "            raise TypeError(\"Pass fiona collection or file path as inundation_polygon\")\n",
    "\n",
    "    # save desired array shape\n",
    "    desired_shape = rem_array.shape\n",
    "\n",
    "    # flatten\n",
    "    rem_array = rem_array.ravel()\n",
    "    catchments_array = catchments_array.ravel()\n",
    "\n",
    "    # create flat outputs\n",
    "    depths_array = rem_array.copy()\n",
    "    inundation_array = catchments_array.copy()\n",
    "\n",
    "    # reset output values\n",
    "    depths_array[depths_array != depths_profile['nodata']] = 0\n",
    "    inundation_array[inundation_array != inundation_profile['nodata']] = inundation_array[inundation_array != inundation_profile['nodata']] * -1\n",
    "\n",
    "    # make output arrays\n",
    "    inundation_array,depths_array = __go_fast_mapping(rem_array,catchments_array,catchmentStagesDict,inundation_array,depths_array)\n",
    "\n",
    "    # reshape output arrays\n",
    "    inundation_array = inundation_array.reshape(desired_shape)\n",
    "    depths_array = depths_array.reshape(desired_shape)\n",
    "\n",
    "    # write out inundation and depth rasters\n",
    "    if isinstance(inundation_raster,DatasetWriter):\n",
    "        inundation_raster.write(inundation_array,indexes=1)\n",
    "    if isinstance(depths,DatasetWriter):\n",
    "        depths.write(depths_array,indexes=1)\n",
    "\n",
    "    # polygonize inundation\n",
    "    if isinstance(inundation_polygon,fiona.Collection):\n",
    "\n",
    "        # make generator for inundation polygons\n",
    "        inundation_polygon_generator = shapes(inundation_array,mask=inundation_array>0,connectivity=8,transform=window_transform)\n",
    "        \n",
    "        # generate records\n",
    "        records = []\n",
    "        for i,(g,h) in enumerate(inundation_polygon_generator):\n",
    "            record = dict()\n",
    "            record['geometry'] = g\n",
    "            record['properties'] = {'HydroID' : int(h)}\n",
    "            records += [record]\n",
    "        \n",
    "        # write out\n",
    "        inundation_polygon.writerecords(records)\n",
    "\n",
    "    if isinstance(depths,DatasetWriter): depths.close()\n",
    "    if isinstance(inundation_raster,DatasetWriter): inundation_raster.close()\n",
    "    if isinstance(inundation_polygon,fiona.Collection): inundation_polygon.close()\n",
    "    #if isinstance(hucs,fiona.Collection): inundation_polygon.close()\n",
    "\n",
    "    # return file names of outputs for aggregation. Handle Nones\n",
    "    try:\n",
    "        ir_name = inundation_raster.name\n",
    "    except AttributeError:\n",
    "        ir_name = None\n",
    "\n",
    "    try:\n",
    "        d_name = depths.name\n",
    "    except AttributeError:\n",
    "        d_name = None\n",
    "\n",
    "    try:\n",
    "        ip_name = inundation_polygon.path\n",
    "    except AttributeError:\n",
    "        ip_name = None\n",
    "    \n",
    "    #print(ir_name)\n",
    "    #yield(ir_name,d_name,ip_name)\n",
    "    \n",
    "    if isinstance(depths,DatasetWriter): depths.close()\n",
    "    if isinstance(inundation_raster,DatasetWriter): inundation_raster.close()\n",
    "    if isinstance(inundation_polygon,fiona.Collection): inundation_polygon.close()\n",
    "\n",
    "    return(ir_name,d_name,ip_name)\n",
    "\n",
    "\n",
    "@njit\n",
    "def __go_fast_mapping(rem,catchments,catchmentStagesDict,inundation,depths):\n",
    "\n",
    "    for i,(r,cm) in enumerate(zip(rem,catchments)):\n",
    "        if cm in catchmentStagesDict:\n",
    "\n",
    "            depth = catchmentStagesDict[cm] - r\n",
    "            depths[i] = max(depth,0) # set negative depths to 0\n",
    "\n",
    "            if depths[i] > 0: # set positive depths to positive\n",
    "                inundation[i] *= -1\n",
    "            #else: # set positive depths to value of positive catchment value\n",
    "                #inundation[i] = cm\n",
    "\n",
    "    return(inundation,depths)\n",
    "\n",
    "\n",
    "def __make_windows_generator(rem, \n",
    "                             catchments,\n",
    "                             catchment_poly,\n",
    "                             mask_type,\n",
    "                             catchmentStagesDict,\n",
    "                             inundation_raster,\n",
    "                             inundation_polygon,\n",
    "                             depths,\n",
    "                             out_raster_profile,\n",
    "                             out_vector_profile,\n",
    "                             quiet,\n",
    "                             hucs = None,\n",
    "                             hucSet = None):\n",
    "\n",
    "    \n",
    "    if hucs is not None:\n",
    "\n",
    "        # get attribute name for HUC column\n",
    "        for huc in hucs:\n",
    "            for hucColName in huc['properties'].keys():\n",
    "                if 'HUC' in hucColName:\n",
    "                    hucSize = int(hucColName[-1])\n",
    "                    break\n",
    "            break\n",
    "\n",
    "        # make windows\n",
    "        for huc in hucs:\n",
    "\n",
    "            # returns hucCode if current huc is in hucSet (at least starts with)\n",
    "            def __return_huc_in_hucSet(hucCode,hucSet):\n",
    "\n",
    "                for hs in hucSet:\n",
    "                    if hs.startswith(hucCode):\n",
    "                        return(hucCode)\n",
    "\n",
    "                return(None)\n",
    "\n",
    "            if  __return_huc_in_hucSet(huc['properties'][hucColName],hucSet) is None:\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                if mask_type == \"huc\":\n",
    "                    #window = geometry_window(rem,shape(huc['geometry']))\n",
    "                    rem_array,window_transform = mask(rem,[shape(huc['geometry'])],crop=True,indexes=1)\n",
    "                    catchments_array,_ = mask(catchments,[shape(huc['geometry'])],crop=True,indexes=1)\n",
    "                elif mask_type == \"filter\":\n",
    "\n",
    "                    # input catchments polygon\n",
    "                    if isinstance(catchment_poly,str):\n",
    "                        catchment_poly=gpd.read_file(catchment_poly)\n",
    "                    elif isinstance(catchment_poly,DatasetReader):\n",
    "                        pass\n",
    "                    else:\n",
    "                        raise TypeError(\"Pass geopandas dataset or filepath for catchment polygons\")\n",
    "\n",
    "                    fossid = huc['properties']['fossid']\n",
    "                    if catchment_poly.comid.dtype != 'str': catchment_poly.comid = catchment_poly.comid.astype(str)\n",
    "                    catchment_poly=catchment_poly[catchment_poly.comid.str.startswith(fossid)]\n",
    "\n",
    "                    rem_array,window_transform = mask(rem,catchment_poly['geometry'],crop=True,indexes=1)\n",
    "                    catchments_array,_ = mask(catchments,catchment_poly['geometry'],crop=True,indexes=1)\n",
    "                    del catchment_poly\n",
    "                elif mask_type is None:\n",
    "                    pass\n",
    "                else:\n",
    "                    print (\"invalid mask type. Options are 'huc' or 'filter'\")\n",
    "            except ValueError: # shape doesn't overlap raster\n",
    "                continue # skip to next HUC\n",
    "\n",
    "            hucCode = huc['properties'][hucColName]\n",
    "\n",
    "            yield (rem_array, catchments_array, rem.crs.wkt,\n",
    "                   window_transform, rem.profile, catchments.profile, hucCode,\n",
    "                   catchmentStagesDict, depths, inundation_raster,\n",
    "                   inundation_polygon, out_raster_profile, out_vector_profile, quiet)\n",
    "\n",
    "    else:\n",
    "        hucCode = None\n",
    "       #window = Window(col_off=0,row_off=0,width=rem.width,height=rem.height)\n",
    "\n",
    "        yield (rem.read(1),catchments.read(1),rem.crs.wkt,\n",
    "               rem.transform,rem.profile,catchments.profile,hucCode,\n",
    "               catchmentStagesDict,depths,inundation_raster,\n",
    "               inundation_polygon,out_raster_profile,out_vector_profile,quiet)\n",
    "\n",
    "\n",
    "def __append_huc_code_to_file_name(fileName,hucCode):\n",
    "\n",
    "    if hucCode is None:\n",
    "        return(fileName)\n",
    "\n",
    "    base_file_path,extension = splitext(fileName)\n",
    "\n",
    "    return(\"{}_{}{}\".format(base_file_path,hucCode,extension))\n",
    "\n",
    "\n",
    "def __subset_hydroTable_to_forecast(huc_6_id, hydroTable,forecast,subset_hucs=None):\n",
    "\n",
    "    if isinstance(hydroTable,str):\n",
    "        hydroTable = pd.read_csv(\n",
    "                                 hydroTable,\n",
    "                                 dtype={'CatchId':str,'Stage':float,\n",
    "                                         'Discharge (m3s-1)':float}\n",
    "                                )\n",
    "        hydroTable['feature_id']=hydroTable['CatchId']\n",
    "        hydroTable['HUC']=huc_6_id\n",
    "        huc_error = hydroTable.HUC.unique()\n",
    "        hydroTable.set_index(['HUC','feature_id','CatchId'],inplace=True)\n",
    "\n",
    "    elif isinstance(hydroTable,pd.DataFrame):\n",
    "        pass #consider checking for correct dtypes, indices, and columns\n",
    "    else:\n",
    "        raise TypeError(\"Pass path to hydro-table csv or Pandas DataFrame\")\n",
    "\n",
    "\n",
    "    # raises error if hydroTable is empty due to all segments being lakes\n",
    "    if hydroTable.empty:\n",
    "        raise hydroTableHasOnlyLakes(\"All stream segments in HUC are within lake boundaries.\")\n",
    "\n",
    "\n",
    "    if isinstance(forecast,str):\n",
    "\n",
    "        try:\n",
    "            forecast = pd.read_csv(\n",
    "                                   forecast,\n",
    "                                   dtype={'feature_id' : str , 'discharge' : float}\n",
    "                                  )\n",
    "            forecast.set_index('feature_id',inplace=True)\n",
    "        except UnicodeDecodeError:\n",
    "            forecast = read_nwm_forecast_file(forecast)\n",
    "    \n",
    "    elif isinstance(forecast,pd.DataFrame):\n",
    "        pass # consider checking for dtypes, indices, and columns\n",
    "    else:\n",
    "        raise TypeError(\"Pass path to forecast file csv or Pandas DataFrame\")\n",
    "\n",
    "    # susbset hucs if passed\n",
    "    if subset_hucs is not None:\n",
    "        if isinstance(subset_hucs,list):\n",
    "            if len(subset_hucs) == 1:\n",
    "                try:\n",
    "                    subset_hucs = open(subset_hucs[0]).read().split('\\n')\n",
    "                except FileNotFoundError:\n",
    "                    pass\n",
    "        elif isinstance(subset_hucs,str):\n",
    "                try:\n",
    "                    subset_hucs = open(subset_hucs).read().split('\\n')\n",
    "                except FileNotFoundError:\n",
    "                    subset_hucs = [subset_hucs]\n",
    "\n",
    "    if not hydroTable.empty:\n",
    "\n",
    "        if isinstance(forecast,str):\n",
    "            forecast = pd.read_csv(\n",
    "                                   forecast,\n",
    "                                   dtype={'feature_id' : str , 'discharge' : float}\n",
    "                                  )\n",
    "            forecast.set_index('feature_id',inplace=True)\n",
    "        elif isinstance(forecast,pd.DataFrame):\n",
    "            pass # consider checking for dtypes, indices, and columns\n",
    "        else:\n",
    "            raise TypeError(\"Pass path to forecast file csv or Pandas DataFrame\")\n",
    "\n",
    "        # susbset hucs if passed\n",
    "        if subset_hucs is not None:\n",
    "            if isinstance(subset_hucs,list):\n",
    "                if len(subset_hucs) == 1:\n",
    "                    try:\n",
    "                        subset_hucs = open(subset_hucs[0]).read().split('\\n')\n",
    "                    except FileNotFoundError:\n",
    "                        pass\n",
    "            elif isinstance(subset_hucs,str):\n",
    "                    try:\n",
    "                        subset_hucs = open(subset_hucs).read().split('\\n')\n",
    "                    except FileNotFoundError:\n",
    "                        subset_hucs = [subset_hucs]\n",
    "\n",
    "            # subsets HUCS\n",
    "            subset_hucs_orig = subset_hucs.copy() ; subset_hucs = []\n",
    "            for huc in np.unique(hydroTable.index.get_level_values('HUC')):\n",
    "                for sh in subset_hucs_orig:\n",
    "                    if huc.startswith(sh):\n",
    "                        subset_hucs += [huc]\n",
    "\n",
    "            hydroTable = hydroTable[np.in1d(hydroTable.index.get_level_values('HUC'), subset_hucs)]\n",
    "\n",
    "    # join tables\n",
    "    try:\n",
    "        hydroTable = hydroTable.join(forecast,on=['feature_id'],how='inner')\n",
    "    except AttributeError:\n",
    "        #print(\"FORECAST ERROR\")\n",
    "        raise NoForecastFound(\"No forecast value found for the passed feature_ids in the Hydro-Table\")\n",
    "\n",
    "    else:\n",
    "\n",
    "        # initialize dictionary\n",
    "        catchmentStagesDict = typed.Dict.empty(types.int32,types.float64)\n",
    "\n",
    "        # interpolate stages\n",
    "        for hid,sub_table in hydroTable.groupby(level='CatchId'):\n",
    "\n",
    "            interpolated_stage = np.interp(sub_table.loc[:,'discharge'].unique(),sub_table.loc[:,'Discharge (m3s-1)'],sub_table.loc[:,'Stage'])\n",
    "\n",
    "            # add this interpolated stage to catchment stages dict\n",
    "            h = round(interpolated_stage[0],4)\n",
    "\n",
    "            hid = types.int32(hid) ; h = types.float32(h)\n",
    "            catchmentStagesDict[hid] = h\n",
    "\n",
    "        # huc set\n",
    "        hucSet = [str(i) for i in hydroTable.index.get_level_values('HUC').unique().to_list()]\n",
    "\n",
    "        return(catchmentStagesDict,hucSet)\n",
    "\n",
    "\n",
    "def read_nwm_forecast_file(forecast_file,rename_headers=True):\n",
    "        \n",
    "    \"\"\" Reads NWM netcdf comp files and converts to forecast data frame \"\"\"\n",
    "\n",
    "    flows_nc = xr.open_dataset(forecast_file,decode_cf='feature_id',engine='netcdf4')\n",
    "    \n",
    "    flows_df = flows_nc.to_dataframe()\n",
    "    flows_df.reset_index(inplace=True)\n",
    "    \n",
    "    flows_df = flows_df[['streamflow','feature_id']]\n",
    "    \n",
    "    if rename_headers:\n",
    "        flows_df = flows_df.rename(columns={\"streamflow\": \"discharge\"})\n",
    "\n",
    "    convert_dict = {'feature_id': str,'discharge': float}\n",
    "    flows_df = flows_df.astype(convert_dict)\n",
    "    \n",
    "    flows_df.set_index('feature_id',inplace=True,drop=True)\n",
    "\n",
    "    flows_df.dropna(inplace=True)\n",
    "\n",
    "    return(flows_df)\n",
    "\n",
    "\n",
    "def __vprint(message,verbose):\n",
    "    if verbose:\n",
    "        print(message)\n",
    "\n",
    "def create_src_subset_csv(hydro_table,catchmentStagesDict,src_table):\n",
    "    src_df = pd.DataFrame.from_dict(catchmentStagesDict, orient='index')\n",
    "    src_df.reset_index(inplace=True)\n",
    "    src_df.columns = ['CatchId','stage_inund']\n",
    "    df_htable = pd.read_csv(hydro_table,dtype={'CatchId': int})\n",
    "    df_htable = df_htable.merge(src_df,how='left',on='CatchId')\n",
    "    df_htable['find_match'] = (df_htable['Stage'] - df_htable['stage_inund']).abs()\n",
    "    df_htable = df_htable.loc[df_htable.groupby('CatchId')['find_match'].idxmin()].reset_index(drop=True)\n",
    "    df_htable.to_csv(src_table,index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17594ec8-655c-41c9-ac46-ed2e102e07d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "### this is outdated'\n",
    "\n",
    "rem='/home/jovyan/data/aghanghas/inundation_map/hand_src/010802hand.tif'\n",
    "catchments='/home/jovyan/data/aghanghas/inundation_map/hand_src/010802catchhuc.tif'\n",
    "catchment_poly='/home/jovyan/data/aghanghas/inundation_map/hand_src/010802_catch.sqlite'\n",
    "hydro_table='/home/jovyan/data/aghanghas/inundation_map/hand_src/hydrogeo-fulltable-010802.csv'\n",
    "\n",
    "forecast_folder='/home/jovyan/data/aghanghas/inundation_map/nwm-v2-1/'\n",
    "#forecast_list=[forecast_folder+f for f in os.listdir(forecast_folder)]\n",
    "\n",
    "forecast='/home/jovyan/data/aghanghas/inundation_map/nwm-v2-1/201210300000.CHRTOUT_DOMAIN1.comp'\n",
    "#forecast='/home/jovyan/data/aghanghas/inundation_map/nwm-v2-1/201211071300.CHRTOUT_DOMAIN1.comp'\n",
    "\n",
    "out_path_init='/home/jovyan/data/aghanghas/inundation_map/outputs/010802-'\n",
    "#indundation_list=[out_path_init+f[0:11]+'_inundation_map.tif' for f in os.listdir(forecast_folder)]\n",
    "indundation_raster='/home/jovyan/data/aghanghas/inundation_map/outputs/010802-2012103000_inundation_map.tif'\n",
    "depth_raster='/home/jovyan/data/aghanghas/inundation_map/outputs/010802-2012103000_depth_map.tif'\n",
    "\n",
    "#indundation_raster='/home/jovyan/data/aghanghas/inundation_map/hand_src/010802indundate.tif'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e9fa395d-de96-4d0c-984d-90a39cef06ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jovyan/data/aghanghas/inundation_map/hand_src//030202/030202hand.tif\n",
      "/home/jovyan/data/aghanghas/inundation_map/hand_src//030202/030202catchhuc.tif\n",
      "/home/jovyan/data/aghanghas/inundation_map/hand_src//030202/030202_catch.sqlite\n",
      "/home/jovyan/data/aghanghas/inundation_map/nwm-v2-1/201809180000.CHRTOUT_DOMAIN1.comp\n",
      "030202\n",
      "/home/jovyan/data/aghanghas/inundation_map/outputs/030202_201809180000_inundation_map.tif\n",
      "/home/jovyan/data/aghanghas/inundation_map/outputs/030202_201809180000_depth_map.tif\n",
      "/home/jovyan/data/aghanghas/inundation_map/hand_src//030202/hydrogeo-fulltable-030202.csv\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "Input \u001b[0;32mIn [2]\u001b[0m, in \u001b[0;36m__subset_hydroTable_to_forecast\u001b[0;34m(huc_6_id, hydroTable, forecast, subset_hucs)\u001b[0m\n\u001b[1;32m    481\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 482\u001b[0m     forecast \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    483\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mforecast\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    484\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfeature_id\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdischarge\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mfloat\u001b[39;49m\u001b[43m}\u001b[49m\n\u001b[1;32m    485\u001b[0m \u001b[43m                          \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    486\u001b[0m     forecast\u001b[38;5;241m.\u001b[39mset_index(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfeature_id\u001b[39m\u001b[38;5;124m'\u001b[39m,inplace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/pandas/util/_decorators.py:311\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    306\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    307\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39marguments),\n\u001b[1;32m    308\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[1;32m    309\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mstacklevel,\n\u001b[1;32m    310\u001b[0m     )\n\u001b[0;32m--> 311\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/pandas/io/parsers/readers.py:680\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    678\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 680\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/pandas/io/parsers/readers.py:575\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    574\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 575\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    577\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/pandas/io/parsers/readers.py:933\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    932\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 933\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/pandas/io/parsers/readers.py:1235\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1234\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1235\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmapping\u001b[49m\u001b[43m[\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1236\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/pandas/io/parsers/c_parser_wrapper.py:75\u001b[0m, in \u001b[0;36mCParserWrapper.__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m     74\u001b[0m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m ensure_dtype_objs(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m---> 75\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reader \u001b[38;5;241m=\u001b[39m \u001b[43mparsers\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTextReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39munnamed_cols \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reader\u001b[38;5;241m.\u001b[39munnamed_cols\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/pandas/_libs/parsers.pyx:544\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/pandas/_libs/parsers.pyx:633\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._get_header\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/pandas/_libs/parsers.pyx:847\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/pandas/_libs/parsers.pyx:1952\u001b[0m, in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mUnicodeDecodeError\u001b[0m: 'utf-8' codec can't decode byte 0x89 in position 0: invalid start byte",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [4]\u001b[0m, in \u001b[0;36m<cell line: 10>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28mprint\u001b[39m(depth_raster)\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28mprint\u001b[39m(hydro_table)\n\u001b[0;32m---> 28\u001b[0m \u001b[43minundate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrem\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrem\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcatchments\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcatchments\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcatchment_poly\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcatchment_poly\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhydro_table\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhydro_table\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforecast\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforecast\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[43m     \u001b[49m\u001b[43mmask_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfilter\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mhuc_6_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhuc_6_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhucs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhucs_layerName\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[43m     \u001b[49m\u001b[43msubset_hucs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_workers\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maggregate\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[43m     \u001b[49m\u001b[43minundation_raster\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mindundation_raster\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minundation_polygon\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[43m     \u001b[49m\u001b[43mdepths\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdepth_raster\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout_raster_profile\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout_vector_profile\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[43m     \u001b[49m\u001b[43msrc_table\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquiet\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [2]\u001b[0m, in \u001b[0;36minundate\u001b[0;34m(rem, catchments, catchment_poly, hydro_table, forecast, mask_type, huc_6_id, hucs, hucs_layerName, subset_hucs, num_workers, aggregate, inundation_raster, inundation_polygon, depths, out_raster_profile, out_vector_profile, src_table, quiet)\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[38;5;66;03m# check for matching projections\u001b[39;00m\n\u001b[1;32m    144\u001b[0m \u001b[38;5;66;03m#assert to_string(hucs.crs) == rem.crs.to_proj4() == catchments.crs.to_proj4(), \"REM, Catchment, and HUCS CRS definitions must match\"\u001b[39;00m\n\u001b[1;32m    145\u001b[0m \n\u001b[1;32m    146\u001b[0m \u001b[38;5;66;03m# catchment stages dictionary\u001b[39;00m\n\u001b[1;32m    147\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m hydro_table \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 148\u001b[0m     catchmentStagesDict,hucSet \u001b[38;5;241m=\u001b[39m \u001b[43m__subset_hydroTable_to_forecast\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhuc_6_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43mhydro_table\u001b[49m\u001b[43m,\u001b[49m\u001b[43mforecast\u001b[49m\u001b[43m,\u001b[49m\u001b[43msubset_hucs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    150\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPass hydro table csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Input \u001b[0;32mIn [2]\u001b[0m, in \u001b[0;36m__subset_hydroTable_to_forecast\u001b[0;34m(huc_6_id, hydroTable, forecast, subset_hucs)\u001b[0m\n\u001b[1;32m    486\u001b[0m         forecast\u001b[38;5;241m.\u001b[39mset_index(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfeature_id\u001b[39m\u001b[38;5;124m'\u001b[39m,inplace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    487\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mUnicodeDecodeError\u001b[39;00m:\n\u001b[0;32m--> 488\u001b[0m         forecast \u001b[38;5;241m=\u001b[39m \u001b[43mread_nwm_forecast_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mforecast\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    490\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(forecast,pd\u001b[38;5;241m.\u001b[39mDataFrame):\n\u001b[1;32m    491\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m \u001b[38;5;66;03m# consider checking for dtypes, indices, and columns\u001b[39;00m\n",
      "Input \u001b[0;32mIn [2]\u001b[0m, in \u001b[0;36mread_nwm_forecast_file\u001b[0;34m(forecast_file, rename_headers)\u001b[0m\n\u001b[1;32m    574\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mread_nwm_forecast_file\u001b[39m(forecast_file,rename_headers\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m    576\u001b[0m     \u001b[38;5;124;03m\"\"\" Reads NWM netcdf comp files and converts to forecast data frame \"\"\"\u001b[39;00m\n\u001b[0;32m--> 578\u001b[0m     flows_nc \u001b[38;5;241m=\u001b[39m \u001b[43mxr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mforecast_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43mdecode_cf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfeature_id\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mengine\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mnetcdf4\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    580\u001b[0m     flows_df \u001b[38;5;241m=\u001b[39m flows_nc\u001b[38;5;241m.\u001b[39mto_dataframe()\n\u001b[1;32m    581\u001b[0m     flows_df\u001b[38;5;241m.\u001b[39mreset_index(inplace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/xarray/backends/api.py:495\u001b[0m, in \u001b[0;36mopen_dataset\u001b[0;34m(filename_or_obj, engine, chunks, cache, decode_cf, mask_and_scale, decode_times, decode_timedelta, use_cftime, concat_characters, decode_coords, drop_variables, backend_kwargs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    483\u001b[0m decoders \u001b[38;5;241m=\u001b[39m _resolve_decoders_kwargs(\n\u001b[1;32m    484\u001b[0m     decode_cf,\n\u001b[1;32m    485\u001b[0m     open_backend_dataset_parameters\u001b[38;5;241m=\u001b[39mbackend\u001b[38;5;241m.\u001b[39mopen_dataset_parameters,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    491\u001b[0m     decode_coords\u001b[38;5;241m=\u001b[39mdecode_coords,\n\u001b[1;32m    492\u001b[0m )\n\u001b[1;32m    494\u001b[0m overwrite_encoded_chunks \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moverwrite_encoded_chunks\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m--> 495\u001b[0m backend_ds \u001b[38;5;241m=\u001b[39m \u001b[43mbackend\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen_dataset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    496\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfilename_or_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    497\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdrop_variables\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdrop_variables\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    498\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mdecoders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    499\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    500\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    501\u001b[0m ds \u001b[38;5;241m=\u001b[39m _dataset_from_backend_dataset(\n\u001b[1;32m    502\u001b[0m     backend_ds,\n\u001b[1;32m    503\u001b[0m     filename_or_obj,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    510\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    511\u001b[0m )\n\u001b[1;32m    512\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ds\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/xarray/backends/netCDF4_.py:567\u001b[0m, in \u001b[0;36mNetCDF4BackendEntrypoint.open_dataset\u001b[0;34m(self, filename_or_obj, mask_and_scale, decode_times, concat_characters, decode_coords, drop_variables, use_cftime, decode_timedelta, group, mode, format, clobber, diskless, persist, lock, autoclose)\u001b[0m\n\u001b[1;32m    565\u001b[0m store_entrypoint \u001b[38;5;241m=\u001b[39m StoreBackendEntrypoint()\n\u001b[1;32m    566\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m close_on_error(store):\n\u001b[0;32m--> 567\u001b[0m     ds \u001b[38;5;241m=\u001b[39m \u001b[43mstore_entrypoint\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen_dataset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    568\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstore\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    569\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmask_and_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmask_and_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    570\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdecode_times\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecode_times\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    571\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconcat_characters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconcat_characters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    572\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdecode_coords\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecode_coords\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    573\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdrop_variables\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdrop_variables\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    574\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cftime\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cftime\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    575\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdecode_timedelta\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecode_timedelta\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    576\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    577\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ds\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/xarray/backends/store.py:39\u001b[0m, in \u001b[0;36mStoreBackendEntrypoint.open_dataset\u001b[0;34m(self, store, mask_and_scale, decode_times, concat_characters, decode_coords, drop_variables, use_cftime, decode_timedelta)\u001b[0m\n\u001b[1;32m     25\u001b[0m encoding \u001b[38;5;241m=\u001b[39m store\u001b[38;5;241m.\u001b[39mget_encoding()\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28mvars\u001b[39m, attrs, coord_names \u001b[38;5;241m=\u001b[39m conventions\u001b[38;5;241m.\u001b[39mdecode_cf_variables(\n\u001b[1;32m     28\u001b[0m     \u001b[38;5;28mvars\u001b[39m,\n\u001b[1;32m     29\u001b[0m     attrs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     36\u001b[0m     decode_timedelta\u001b[38;5;241m=\u001b[39mdecode_timedelta,\n\u001b[1;32m     37\u001b[0m )\n\u001b[0;32m---> 39\u001b[0m ds \u001b[38;5;241m=\u001b[39m \u001b[43mDataset\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mvars\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     40\u001b[0m ds \u001b[38;5;241m=\u001b[39m ds\u001b[38;5;241m.\u001b[39mset_coords(coord_names\u001b[38;5;241m.\u001b[39mintersection(\u001b[38;5;28mvars\u001b[39m))\n\u001b[1;32m     41\u001b[0m ds\u001b[38;5;241m.\u001b[39mset_close(store\u001b[38;5;241m.\u001b[39mclose)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/xarray/core/dataset.py:750\u001b[0m, in \u001b[0;36mDataset.__init__\u001b[0;34m(self, data_vars, coords, attrs)\u001b[0m\n\u001b[1;32m    747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(coords, Dataset):\n\u001b[1;32m    748\u001b[0m     coords \u001b[38;5;241m=\u001b[39m coords\u001b[38;5;241m.\u001b[39mvariables\n\u001b[0;32m--> 750\u001b[0m variables, coord_names, dims, indexes, _ \u001b[38;5;241m=\u001b[39m \u001b[43mmerge_data_and_coords\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    751\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_vars\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcoords\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcompat\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbroadcast_equals\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m    752\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    754\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_attrs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(attrs) \u001b[38;5;28;01mif\u001b[39;00m attrs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    755\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_close \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/xarray/core/merge.py:483\u001b[0m, in \u001b[0;36mmerge_data_and_coords\u001b[0;34m(data, coords, compat, join)\u001b[0m\n\u001b[1;32m    481\u001b[0m explicit_coords \u001b[38;5;241m=\u001b[39m coords\u001b[38;5;241m.\u001b[39mkeys()\n\u001b[1;32m    482\u001b[0m indexes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(_extract_indexes_from_coords(coords))\n\u001b[0;32m--> 483\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmerge_core\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    484\u001b[0m \u001b[43m    \u001b[49m\u001b[43mobjects\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcompat\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjoin\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexplicit_coords\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexplicit_coords\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindexes\u001b[49m\n\u001b[1;32m    485\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/xarray/core/merge.py:632\u001b[0m, in \u001b[0;36mmerge_core\u001b[0;34m(objects, compat, join, combine_attrs, priority_arg, explicit_coords, indexes, fill_value)\u001b[0m\n\u001b[1;32m    628\u001b[0m coerced \u001b[38;5;241m=\u001b[39m coerce_pandas_values(objects)\n\u001b[1;32m    629\u001b[0m aligned \u001b[38;5;241m=\u001b[39m deep_align(\n\u001b[1;32m    630\u001b[0m     coerced, join\u001b[38;5;241m=\u001b[39mjoin, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, indexes\u001b[38;5;241m=\u001b[39mindexes, fill_value\u001b[38;5;241m=\u001b[39mfill_value\n\u001b[1;32m    631\u001b[0m )\n\u001b[0;32m--> 632\u001b[0m collected \u001b[38;5;241m=\u001b[39m \u001b[43mcollect_variables_and_indexes\u001b[49m\u001b[43m(\u001b[49m\u001b[43maligned\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    634\u001b[0m prioritized \u001b[38;5;241m=\u001b[39m _get_priority_vars_and_indexes(aligned, priority_arg, compat\u001b[38;5;241m=\u001b[39mcompat)\n\u001b[1;32m    635\u001b[0m variables, out_indexes \u001b[38;5;241m=\u001b[39m merge_collected(\n\u001b[1;32m    636\u001b[0m     collected, prioritized, compat\u001b[38;5;241m=\u001b[39mcompat, combine_attrs\u001b[38;5;241m=\u001b[39mcombine_attrs\n\u001b[1;32m    637\u001b[0m )\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/xarray/core/merge.py:291\u001b[0m, in \u001b[0;36mcollect_variables_and_indexes\u001b[0;34m(list_of_mappings)\u001b[0m\n\u001b[1;32m    288\u001b[0m     indexes\u001b[38;5;241m.\u001b[39mpop(name, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    289\u001b[0m     append_all(coords, indexes)\n\u001b[0;32m--> 291\u001b[0m variable \u001b[38;5;241m=\u001b[39m \u001b[43mas_variable\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvariable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    293\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m variable\u001b[38;5;241m.\u001b[39mdims \u001b[38;5;241m==\u001b[39m (name,):\n\u001b[1;32m    294\u001b[0m     idx_variable \u001b[38;5;241m=\u001b[39m variable\u001b[38;5;241m.\u001b[39mto_index_variable()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/xarray/core/variable.py:154\u001b[0m, in \u001b[0;36mas_variable\u001b[0;34m(obj, name)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m obj\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    149\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m MissingDimensionsError(\n\u001b[1;32m    150\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m has more than 1-dimension and the same name as one of its \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    151\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdimensions \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mobj\u001b[38;5;241m.\u001b[39mdims\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m. xarray disallows such variables because they \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    152\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconflict with the coordinates used to label dimensions.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    153\u001b[0m         )\n\u001b[0;32m--> 154\u001b[0m     obj \u001b[38;5;241m=\u001b[39m \u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_index_variable\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    156\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m obj\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/xarray/core/variable.py:528\u001b[0m, in \u001b[0;36mVariable.to_index_variable\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    526\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mto_index_variable\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    527\u001b[0m     \u001b[38;5;124;03m\"\"\"Return this variable as an xarray.IndexVariable\"\"\"\u001b[39;00m\n\u001b[0;32m--> 528\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mIndexVariable\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    529\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdims\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_attrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_encoding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfastpath\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[1;32m    530\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/xarray/core/variable.py:2672\u001b[0m, in \u001b[0;36mIndexVariable.__init__\u001b[0;34m(self, dims, data, attrs, encoding, fastpath)\u001b[0m\n\u001b[1;32m   2670\u001b[0m \u001b[38;5;66;03m# Unlike in Variable, always eagerly load values into memory\u001b[39;00m\n\u001b[1;32m   2671\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data, PandasIndexingAdapter):\n\u001b[0;32m-> 2672\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data \u001b[38;5;241m=\u001b[39m \u001b[43mPandasIndexingAdapter\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_data\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/xarray/core/indexing.py:1272\u001b[0m, in \u001b[0;36mPandasIndexingAdapter.__init__\u001b[0;34m(self, array, dtype)\u001b[0m\n\u001b[1;32m   1271\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, array: pd\u001b[38;5;241m.\u001b[39mIndex, dtype: DTypeLike \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m-> 1272\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39marray \u001b[38;5;241m=\u001b[39m \u001b[43mutils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msafe_cast_to_index\u001b[49m\u001b[43m(\u001b[49m\u001b[43marray\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1274\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1275\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(array, pd\u001b[38;5;241m.\u001b[39mPeriodIndex):\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/xarray/core/utils.py:115\u001b[0m, in \u001b[0;36msafe_cast_to_index\u001b[0;34m(array)\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(array, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m array\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mkind \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mO\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    114\u001b[0m         kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mobject\u001b[39m\n\u001b[0;32m--> 115\u001b[0m     index \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mIndex(\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43masarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43marray\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _maybe_cast_to_cftimeindex(index)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/xarray/core/indexing.py:423\u001b[0m, in \u001b[0;36mLazilyIndexedArray.__array__\u001b[0;34m(self, dtype)\u001b[0m\n\u001b[1;32m    421\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__array__\u001b[39m(\u001b[38;5;28mself\u001b[39m, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    422\u001b[0m     array \u001b[38;5;241m=\u001b[39m as_indexable(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39marray)\n\u001b[0;32m--> 423\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43masarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43marray\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkey\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/xarray/coding/variables.py:70\u001b[0m, in \u001b[0;36m_ElementwiseFunctionArray.__array__\u001b[0;34m(self, dtype)\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__array__\u001b[39m(\u001b[38;5;28mself\u001b[39m, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/xarray/coding/times.py:261\u001b[0m, in \u001b[0;36mdecode_cf_datetime\u001b[0;34m(num_dates, units, calendar, use_cftime)\u001b[0m\n\u001b[1;32m    259\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cftime \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    260\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 261\u001b[0m         dates \u001b[38;5;241m=\u001b[39m \u001b[43m_decode_datetime_with_pandas\u001b[49m\u001b[43m(\u001b[49m\u001b[43mflat_num_dates\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43munits\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcalendar\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    262\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mKeyError\u001b[39;00m, OutOfBoundsDatetime, \u001b[38;5;167;01mOverflowError\u001b[39;00m):\n\u001b[1;32m    263\u001b[0m         dates \u001b[38;5;241m=\u001b[39m _decode_datetime_with_cftime(\n\u001b[1;32m    264\u001b[0m             flat_num_dates\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mfloat\u001b[39m), units, calendar\n\u001b[1;32m    265\u001b[0m         )\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/xarray/coding/times.py:217\u001b[0m, in \u001b[0;36m_decode_datetime_with_pandas\u001b[0;34m(flat_num_dates, units, calendar)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m warnings\u001b[38;5;241m.\u001b[39mcatch_warnings():\n\u001b[1;32m    216\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mfilterwarnings(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minvalid value encountered\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;167;01mRuntimeWarning\u001b[39;00m)\n\u001b[0;32m--> 217\u001b[0m     pd\u001b[38;5;241m.\u001b[39mto_timedelta(\u001b[43mflat_num_dates\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m, delta) \u001b[38;5;241m+\u001b[39m ref_date\n\u001b[1;32m    218\u001b[0m     pd\u001b[38;5;241m.\u001b[39mto_timedelta(flat_num_dates\u001b[38;5;241m.\u001b[39mmax(), delta) \u001b[38;5;241m+\u001b[39m ref_date\n\u001b[1;32m    220\u001b[0m \u001b[38;5;66;03m# To avoid integer overflow when converting to nanosecond units for integer\u001b[39;00m\n\u001b[1;32m    221\u001b[0m \u001b[38;5;66;03m# dtypes smaller than np.int64 cast all integer-dtype arrays to np.int64\u001b[39;00m\n\u001b[1;32m    222\u001b[0m \u001b[38;5;66;03m# (GH 2002).\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/numpy/core/_methods.py:44\u001b[0m, in \u001b[0;36m_amin\u001b[0;34m(a, axis, out, keepdims, initial, where)\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_amin\u001b[39m(a, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, out\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, keepdims\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m     43\u001b[0m           initial\u001b[38;5;241m=\u001b[39m_NoValue, where\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m---> 44\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mumr_minimum\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeepdims\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minitial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwhere\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "huc_6_id_list=['030202']\n",
    "data_folder='/home/jovyan/data/aghanghas/inundation_map/hand_src/'\n",
    "out_folder='/home/jovyan/data/aghanghas/inundation_map/outputs/'\n",
    "\n",
    "start_date='2018-09-18'\n",
    "end_date='2018-09-19'\n",
    "\n",
    "date_list=pd.date_range(start=start_date,end=end_date,freq='H')\n",
    "\n",
    "for t in date_list:\n",
    "    date_time=t.strftime('%Y%m%d%H%M')\n",
    "    forecast='/home/jovyan/data/aghanghas/inundation_map/nwm-v2-1/' + date_time + '.CHRTOUT_DOMAIN1.comp'\n",
    "    for huc_6_id in huc_6_id_list:\n",
    "        rem=data_folder+ '/'+ huc_6_id +'/'+ huc_6_id +'hand.tif'\n",
    "        catchments=data_folder +'/'+ huc_6_id +'/'+ huc_6_id + 'catchhuc.tif'\n",
    "        catchment_poly=data_folder + '/'+ huc_6_id +'/'+ huc_6_id + '_catch.sqlite'\n",
    "        hydro_table = data_folder + '/'+ huc_6_id +'/'+ 'hydrogeo-fulltable-' + huc_6_id + '.csv'\n",
    "        indundation_raster=out_folder + huc_6_id +'_'+ date_time +'_inundation_map.tif'\n",
    "        depth_raster = out_folder + huc_6_id + '_' + date_time + '_depth_map.tif'\n",
    "        print(rem)\n",
    "        print(catchments)\n",
    "        print(catchment_poly)\n",
    "        print(forecast)\n",
    "        print(huc_6_id)\n",
    "        print(indundation_raster)\n",
    "        print(depth_raster)\n",
    "        print(hydro_table)\n",
    "        inundate(rem=rem, catchments=catchments, catchment_poly=catchment_poly, hydro_table=hydro_table, forecast=forecast,\n",
    "             mask_type='filter',huc_6_id=huc_6_id, hucs = None, hucs_layerName = None,\n",
    "             subset_hucs = None, num_workers = 1, aggregate = False, \n",
    "             inundation_raster = indundation_raster, inundation_polygon = None,\n",
    "             depths = depth_raster, out_raster_profile = None, out_vector_profile = None,\n",
    "             src_table = None, quiet = False)\n",
    "        \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "65678400-17a8-4246-8cdb-a6543de1cb0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... complete\n",
      "... complete\n",
      "... complete\n",
      "... complete\n",
      "... complete\n",
      "... complete\n",
      "... complete\n",
      "... complete\n",
      "... complete\n",
      "... complete\n",
      "... complete\n",
      "... complete\n",
      "... complete\n",
      "... complete\n",
      "... complete\n",
      "... complete\n",
      "... complete\n",
      "... complete\n",
      "... complete\n",
      "... complete\n",
      "... complete\n",
      "... complete\n",
      "... complete\n",
      "... complete\n",
      "... complete\n",
      "... complete\n",
      "... complete\n",
      "... complete\n",
      "... complete\n",
      "... complete\n",
      "... complete\n",
      "... complete\n",
      "... complete\n",
      "... complete\n",
      "... complete\n",
      "... complete\n",
      "... complete\n",
      "... complete\n",
      "... complete\n",
      "... complete\n",
      "... complete\n",
      "... complete\n",
      "... complete\n",
      "... complete\n",
      "... complete\n",
      "... complete\n",
      "... complete\n",
      "... complete\n",
      "... complete\n",
      "... complete\n",
      "... complete\n",
      "... complete\n",
      "... complete\n",
      "... complete\n",
      "... complete\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(os.listdir(forecast_folder))):\n",
    "    forecast=forecast_list[i]\n",
    "    indundation_raster=indundation_list[i]\n",
    "    inundate(rem=rem, catchments=catchments, catchment_poly=catchment_poly, hydro_table=hydro_table, forecast=forecast,\n",
    "             mask_type='huc', hucs = None, hucs_layerName = None,\n",
    "             subset_hucs = None, num_workers = 1, aggregate = False, \n",
    "             inundation_raster = indundation_raster, inundation_polygon = None,\n",
    "             depths = None, out_raster_profile = None, out_vector_profile = None,\n",
    "             src_table = None, quiet = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f349bb6f-c9b7-44db-a48f-574d3eca316a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... complete\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(['/home/jovyan/data/aghanghas/inundation_map/outputs/010802-2012103000_inundation_map.tif'],\n",
       " ['/home/jovyan/data/aghanghas/inundation_map/outputs/010802-2012103000_depth_map.tif'],\n",
       " [None])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "    inundate(rem=rem, catchments=catchments, catchment_poly=catchment_poly, hydro_table=hydro_table, forecast=forecast,\n",
    "             mask_type='filter',huc_8_id='010802', hucs = None, hucs_layerName = None,\n",
    "             subset_hucs = None, num_workers = 1, aggregate = False, \n",
    "             inundation_raster = indundation_raster, inundation_polygon = None,\n",
    "             depths = depth_raster, out_raster_profile = None, out_vector_profile = None,\n",
    "             src_table = None, quiet = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "830393e4-b2d2-4dad-97a7-840a1b884842",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
